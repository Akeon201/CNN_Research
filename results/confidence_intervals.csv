Activation Function \ Learning Rate,.001,.0001,.00001
Tanh,"(0.3012931083351148, 0.34964022499821823)","(0.3777102587759331, 0.4232897412240667)","(0.300175716387225, 0.3419576169461082)"
Sigmoid,"(0.33425009722760596, 0.38088323610572744)","(0.38765262439510073, 0.4297473756048992)","(0.2818577369774387, 0.32387559635589463)"
ReLU,"(0.3150135423171414, 0.3637197910161918)","(0.4214058005129622, 0.4677275328203709)","(0.3540056018274844, 0.39839439817251554)"
